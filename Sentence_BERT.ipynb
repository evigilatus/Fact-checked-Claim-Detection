{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "medieval-italy",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/borisovai/.local/lib/python3.8/site-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.3) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "INFO : Load pretrained SentenceTransformer: paraphrase-distilroberta-base-v1\n",
      "INFO : Did not find folder paraphrase-distilroberta-base-v1\n",
      "INFO : Try to download model from server: https://sbert.net/models/paraphrase-distilroberta-base-v1.zip\n",
      "INFO : Load SentenceTransformer from folder: /home/borisovai/.cache/torch/sentence_transformers/sbert.net_models_paraphrase-distilroberta-base-v1\n",
      "INFO : Use pytorch device: cuda\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "from glob import glob\n",
    "from os.path import join, dirname, basename, exists\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append('.')\n",
    "\n",
    "from scorer.main import evaluate\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "random.seed(0)\n",
    "ROOT_DIR = os.getcwd()\n",
    "\n",
    "sbert = SentenceTransformer('paraphrase-distilroberta-base-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "desperate-boulder",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vclaims(dir):\n",
    "    vclaims_fp = glob(f'{dir}/*.json')\n",
    "    vclaims_fp.sort()\n",
    "    vclaims = {}\n",
    "    vclaims_list = []\n",
    "    for vclaim_fp in vclaims_fp:\n",
    "        with open(vclaim_fp) as f:\n",
    "            vclaim = json.load(f)\n",
    "        vclaims[vclaim['vclaim_id']] = vclaim\n",
    "        vclaims_list.append(vclaim)\n",
    "    return vclaims, vclaims_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "assigned-struggle",
   "metadata": {},
   "outputs": [],
   "source": [
    "vclaims, vclaims_list = load_vclaims(\"baselines/politifact-vclaims\")\n",
    "all_iclaims = pd.read_csv(\"baselines/v1/iclaims.queries\", sep='\\t', names=['iclaim_id', 'iclaim'])\n",
    "wanted_iclaim_ids = pd.read_csv(\"baselines/v1/train.tsv\", sep='\\t', names=['iclaim_id', '0', 'vclaim_id', 'relevance'])\n",
    "wanted_iclaim_ids = wanted_iclaim_ids.iclaim_id.tolist()\n",
    "\n",
    "iclaims = []\n",
    "for iclaim_id in wanted_iclaim_ids:\n",
    "    iclaim = all_iclaims.iclaim[all_iclaims.iclaim_id == iclaim_id].iloc[0]\n",
    "    iclaims.append((iclaim_id, iclaim))\n",
    "\n",
    "# index = \"2b-english\"\n",
    "\n",
    "# # options are title, vclaim, text\n",
    "# scores = get_scores(iclaims, vclaims_list, index, search_keys=args.keys, size=args.size)\n",
    "# ngram_baseline_fpath = join(ROOT_DIR,\n",
    "#                                 'baselines/data/subtask_2b_bm25_english_baselines/v1/train.tsv')\n",
    "# formatted_scores = format_scores(scores)\n",
    "# with open(ngram_baseline_fpath, 'w') as f:\n",
    "#     f.write(formatted_scores)\n",
    "#     maps, mrr, precisions = evaluate(args.dev_file_path, ngram_baseline_fpath)\n",
    "# logging.info(f\"S-BERT Baseline for Subtask-{args.subtask}--{args.lang}\")\n",
    "# logging.info(f'All MAP scores on threshold from [1, 3, 5, 10, 20, 50, 1000]. {maps}')\n",
    "# logging.info(f'MRR score {mrr}')\n",
    "# logging.info(f'All P scores on threshold from [1, 3, 5, 10, 20, 50, 1000]. {precisions}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "preceding-discussion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fe3f2fb14a9422ba357c109ce23d4f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fc93d5ee2ab4312b55b11a4cf5da9be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a4d3ad81fcf40d182d09d01833c8934",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0bed176e18645d5947030da43718a90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "iclaim_encodings = []\n",
    "iclaim_encodings.append(sbert.encode(iclaims[0]))\n",
    "iclaim_encodings.append(sbert.encode(iclaims[2]))\n",
    "\n",
    "\n",
    "\n",
    "texts = [vclaim['text'] for vclaim in vclaims_list]\n",
    "encodings_list = []\n",
    "\n",
    "encodings_list.append(sbert.encode(sent_tokenize(texts[30])))\n",
    "encodings_list.append(sbert.encode(sent_tokenize(texts[18])))\n",
    "                           \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "friendly-sunday",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You were very clear that you would not provide government assistance to the U.S. auto companies even if they went through bankruptcy.\n"
     ]
    }
   ],
   "source": [
    "iclaims_count, vclaims_count = len(iclaims), len(vclaims_list)\n",
    "scores = {}\n",
    "\n",
    "# Compute the encodings for all sentences in a text\n",
    "texts = [vclaim['text'] for vclaim in vclaims_list]\n",
    "# iclaims_encodings = [sbert.encode(iclaim) for iclaim in iclaims]\n",
    "# vclaim_encodings = [sbert.encode(sent_tokenize(text)) for text in texts]\n",
    "# logging.info(f\"Geting RM5 scores for {iclaims_count} iclaims and {vclaims_count} vclaims\")\n",
    "\n",
    "print(iclaims[0][1])\n",
    "\n",
    "# for iclaim_id, iclaim in iclaims:\n",
    "    \n",
    "#     score = get_score(iclaims_encodings[iclaims.index(iclaim_id, iclaim)], vclaims_list, vclaim_encodings, index, search_keys=search_keys, size=size)\n",
    "#     scores[iclaim_id] = score\n",
    "    \n",
    "# print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "found-irrigation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.13679329  0.34706703 -0.1844899  ...  0.5020161   0.06291218\n",
      "  -0.18408445]\n",
      " [ 0.02565827  0.25984174  0.21223713 ... -0.24807751 -0.13124554\n",
      "   0.03622852]\n",
      " [ 0.09518003  0.211623    0.00453631 ...  0.12868962 -0.14914145\n",
      "   0.38807908]\n",
      " ...\n",
      " [ 0.36941403  0.45837355  0.06119161 ...  0.2959436  -0.01883373\n",
      "   0.01633267]\n",
      " [ 0.07529093  0.36175445  0.08527803 ...  0.26322654 -0.03113398\n",
      "  -0.16800277]\n",
      " [-0.01616848  0.4336011  -0.04188044 ...  0.30927145 -0.00233852\n",
      "   0.14829345]]\n"
     ]
    }
   ],
   "source": [
    "print(encodings_list[0])\n",
    "# print(sentences[33:34].shape)\n",
    "# test_results = util.semantic_search(query, sentences[6:7])\n",
    "# print(test_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "impossible-glance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: [('vclaim-pol-00001', 0.22464996576309204), ('vclaim-pol-00000', 0.18482476472854614)]\n"
     ]
    }
   ],
   "source": [
    "score = {}\n",
    " \n",
    "for i, encodding in enumerate(encodings_list):\n",
    "        results = util.semantic_search(query, encodding, top_k=5)\n",
    "        result_sum = 0\n",
    "        for j, result in enumerate(results):\n",
    "            result_sum += result[j]['score']\n",
    "        average = result_sum / len(results)\n",
    "        vclaim_id = vclaims_list[i]['vclaim_id']\n",
    "        score[vclaim_id] = average\n",
    "score = sorted(score.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Score:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "superb-birthday",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 0 in argument 0, but got numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-3287428518e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msemantic_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miclaim_encodings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencodings_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# for i, encodding in enumerate(encodings_list):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#         results = util.semantic_search(query, encodding, top_k=5)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sentence_transformers/util.py\u001b[0m in \u001b[0;36msemantic_search\u001b[0;34m(query_embeddings, corpus_embeddings, query_chunk_size, corpus_chunk_size, top_k)\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0mquery_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0mquery_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got numpy.ndarray"
     ]
    }
   ],
   "source": [
    "score = {}\n",
    "results = util.semantic_search(iclaim_encodings, encodings_list, top_k=5)\n",
    "\n",
    "# for i, encodding in enumerate(encodings_list):\n",
    "#         results = util.semantic_search(query, encodding, top_k=5)\n",
    "#         result_sum = 0\n",
    "#         for j, result in enumerate(results):\n",
    "#             result_sum += result[j]['score']\n",
    "#         average = result_sum / len(results)\n",
    "#         vclaim_id = vclaims_list[i]['vclaim_id']\n",
    "#         score[vclaim_id] = average\n",
    "# score = sorted(score.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# print(\"Score:\", score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "essential-metadata",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(iclaims, vclaims_list, index, search_keys, size):\n",
    "    iclaims_count, vclaims_count = len(iclaims), len(vclaims_list)\n",
    "    scores = {}\n",
    "\n",
    "    logging.info(f\"Geting RM5 scores for {iclaims_count} iclaims and {vclaims_count} vclaims\")\n",
    "\n",
    "    for iclaim_id, iclaim in iclaims:\n",
    "        score = get_score(iclaim, vclaims_list, index, search_keys=search_keys, size=size)\n",
    "        scores[iclaim_id] = score\n",
    "    return scores\n",
    "\n",
    "\n",
    "def format_scores(scores):\n",
    "    output_string = ''\n",
    "    for iclaim_id in scores:\n",
    "        for i, (vclaim_id, score) in enumerate(scores[iclaim_id]):\n",
    "            output_string += f\"{iclaim_id}\\tQ0\\t{vclaim_id}\\t{i + 1}\\t{score}\\telasic\\n\"\n",
    "    return output_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quick-migration",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_baselines(args):\n",
    "    if not exists('baselines/data'):\n",
    "        os.mkdir('baselines/data')\n",
    "    vclaims, vclaims_list = load_vclaims(args.vclaims_dir_path)\n",
    "    all_iclaims = pd.read_csv(args.iclaims_file_path, sep='\\t', names=['iclaim_id', 'iclaim'])\n",
    "    wanted_iclaim_ids = pd.read_csv(args.dev_file_path, sep='\\t', names=['iclaim_id', '0', 'vclaim_id', 'relevance'])\n",
    "    wanted_iclaim_ids = wanted_iclaim_ids.iclaim_id.tolist()\n",
    "\n",
    "    iclaims = []\n",
    "    for iclaim_id in wanted_iclaim_ids:\n",
    "        iclaim = all_iclaims.iclaim[all_iclaims.iclaim_id == iclaim_id].iloc[0]\n",
    "        iclaims.append((iclaim_id, iclaim))\n",
    "\n",
    "    index = f\"{args.subtask}-{args.lang}\"\n",
    "\n",
    "    # options are title, vclaim, text\n",
    "    scores = get_scores(iclaims, vclaims_list, index, search_keys=args.keys, size=args.size)\n",
    "    ngram_baseline_fpath = join(ROOT_DIR,\n",
    "                                f'baselines/data/subtask_{args.subtask}_bm25_{args.lang}_{basename(args.dev_file_path)}')\n",
    "    formatted_scores = format_scores(scores)\n",
    "    with open(ngram_baseline_fpath, 'w') as f:\n",
    "        f.write(formatted_scores)\n",
    "    maps, mrr, precisions = evaluate(args.dev_file_path, ngram_baseline_fpath)\n",
    "    logging.info(f\"S-BERT Baseline for Subtask-{args.subtask}--{args.lang}\")\n",
    "    logging.info(f'All MAP scores on threshold from [1, 3, 5, 10, 20, 50, 1000]. {maps}')\n",
    "    logging.info(f'MRR score {mrr}')\n",
    "    logging.info(f'All P scores on threshold from [1, 3, 5, 10, 20, 50, 1000]. {precisions}')\n",
    "\n",
    "\n",
    "# python baselines/bm25.py --train-file-path=baselines/v1/train.tsv --dev-file-path=baselines/v1/train.tsv --vclaims-dir-path=baselines/politifact-vclaims --iclaims-file-path=baselines/v1/iclaims.queries --subtask=2b --lang=english\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--train-file-path\", \"-t\", required=True, type=str,\n",
    "                        help=\"The absolute path to the training data\")\n",
    "    parser.add_argument(\"--dev-file-path\", \"-d\", required=True, type=str,\n",
    "                        help=\"The absolute path to the dev data\")\n",
    "    parser.add_argument(\"--vclaims-dir-path\", \"-v\", required=True, type=str,\n",
    "                        help=\"The absolute path to the directory with the verified claim documents\")\n",
    "    parser.add_argument(\"--iclaims-file-path\", \"-i\", required=True,\n",
    "                        help=\"TSV file with iclaims. Format: iclaim_id iclaim_content\")\n",
    "    parser.add_argument(\"--keys\", \"-k\", default=['vclaim', 'title'],\n",
    "                        help=\"Keys to search in the document\")\n",
    "    parser.add_argument(\"--size\", \"-s\", default=19250,\n",
    "                        help=\"Maximum results extracted for a query\")\n",
    "    parser.add_argument(\"--subtask\", \"-m\", required=True,\n",
    "                        choices=['2a', '2b'],\n",
    "                        help=\"The subtask you want to check the format of.\")\n",
    "    parser.add_argument(\"--lang\", \"-l\", required=True, type=str,\n",
    "                        choices=['arabic', 'english'],\n",
    "                        help=\"The language of the subtask\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    run_baselines(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
