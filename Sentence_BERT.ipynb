{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "medieval-italy",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : Load pretrained SentenceTransformer: paraphrase-distilroberta-base-v1\n",
      "INFO : Did not find folder paraphrase-distilroberta-base-v1\n",
      "INFO : Try to download model from server: https://sbert.net/models/paraphrase-distilroberta-base-v1.zip\n",
      "INFO : Load SentenceTransformer from folder: /home/borisovai/.cache/torch/sentence_transformers/sbert.net_models_paraphrase-distilroberta-base-v1\n",
      "INFO : Use pytorch device: cpu\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "from glob import glob\n",
    "from os.path import join, dirname, basename, exists\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append('.')\n",
    "\n",
    "from scorer.main import evaluate\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "random.seed(0)\n",
    "ROOT_DIR = os.getcwd()\n",
    "\n",
    "sbert = SentenceTransformer('paraphrase-distilroberta-base-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "desperate-boulder",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vclaims(dir):\n",
    "    vclaims_fp = glob(f'{dir}/*.json')\n",
    "    vclaims_fp.sort()\n",
    "    vclaims = {}\n",
    "    vclaims_list = []\n",
    "    for vclaim_fp in vclaims_fp:\n",
    "        with open(vclaim_fp) as f:\n",
    "            vclaim = json.load(f)\n",
    "        vclaims[vclaim['vclaim_id']] = vclaim\n",
    "        vclaims_list.append(vclaim)\n",
    "    return vclaims, vclaims_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "assigned-struggle",
   "metadata": {},
   "outputs": [],
   "source": [
    "vclaims, vclaims_list = load_vclaims(\"baselines/politifact-vclaims\")\n",
    "all_iclaims = pd.read_csv(\"baselines/v1/iclaims.queries\", sep='\\t', names=['iclaim_id', 'iclaim'])\n",
    "wanted_iclaim_ids = pd.read_csv(\"baselines/v1/train.tsv\", sep='\\t', names=['iclaim_id', '0', 'vclaim_id', 'relevance'])\n",
    "wanted_iclaim_ids = wanted_iclaim_ids.iclaim_id.tolist()\n",
    "\n",
    "iclaims = []\n",
    "for iclaim_id in wanted_iclaim_ids:\n",
    "    iclaim = all_iclaims.iclaim[all_iclaims.iclaim_id == iclaim_id].iloc[0]\n",
    "    iclaims.append((iclaim_id, iclaim))\n",
    "\n",
    "# index = \"2b-english\"\n",
    "\n",
    "# # options are title, vclaim, text\n",
    "# scores = get_scores(iclaims, vclaims_list, index, search_keys=args.keys, size=args.size)\n",
    "# ngram_baseline_fpath = join(ROOT_DIR,\n",
    "#                                 'baselines/data/subtask_2b_bm25_english_baselines/v1/train.tsv')\n",
    "# formatted_scores = format_scores(scores)\n",
    "# with open(ngram_baseline_fpath, 'w') as f:\n",
    "#     f.write(formatted_scores)\n",
    "#     maps, mrr, precisions = evaluate(args.dev_file_path, ngram_baseline_fpath)\n",
    "# logging.info(f\"S-BERT Baseline for Subtask-{args.subtask}--{args.lang}\")\n",
    "# logging.info(f'All MAP scores on threshold from [1, 3, 5, 10, 20, 50, 1000]. {maps}')\n",
    "# logging.info(f'MRR score {mrr}')\n",
    "# logging.info(f'All P scores on threshold from [1, 3, 5, 10, 20, 50, 1000]. {precisions}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "preceding-discussion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fae53f412dce4d2288cc049eb40cdcb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "801f663c56d9446397518c92375dcb3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70605557b58c456484ca3c8dd16cf380",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = sbert.encode(iclaim)\n",
    "texts = [vclaim['text'] for vclaim in vclaims_list]\n",
    "encoddings_list = []\n",
    "\n",
    "encoddings_list.append(sbert.encode(sent_tokenize(texts[0])))\n",
    "encoddings_list.append(sbert.encode(sent_tokenize(texts[1])))\n",
    "                           \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "found-irrigation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.13679294  0.34706733 -0.18449005 ...  0.5020159   0.06291248\n",
      "  -0.18408431]\n",
      " [ 0.02565829  0.25984177  0.21223696 ... -0.24807763 -0.13124526\n",
      "   0.03622891]\n",
      " [ 0.09517999  0.21162288  0.00453614 ...  0.12868911 -0.14914143\n",
      "   0.38807896]\n",
      " ...\n",
      " [ 0.36941418  0.45837346  0.06119155 ...  0.29594317 -0.01883352\n",
      "   0.01633249]\n",
      " [ 0.07529091  0.3617548   0.0852778  ...  0.2632263  -0.03113406\n",
      "  -0.16800287]\n",
      " [-0.01616891  0.43360144 -0.04188025 ...  0.30927122 -0.00233839\n",
      "   0.14829366]]\n"
     ]
    }
   ],
   "source": [
    "print(encoddings_list[0])\n",
    "# print(sentences[33:34].shape)\n",
    "# test_results = util.semantic_search(query, sentences[6:7])\n",
    "# print(test_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "willing-throw",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43, 768)\n",
      "[{'corpus_id': 6, 'score': 0.16315478}, {'corpus_id': 13, 'score': 0.15716958}, {'corpus_id': 38, 'score': 0.15340334}, {'corpus_id': 18, 'score': 0.15297776}, {'corpus_id': 23, 'score': 0.15165034}]\n",
      "(32, 768)\n",
      "[{'corpus_id': 29, 'score': 0.07074496}, {'corpus_id': 5, 'score': 0.061870046}, {'corpus_id': 11, 'score': 0.054680146}, {'corpus_id': 19, 'score': 0.05001956}, {'corpus_id': 27, 'score': 0.049646538}]\n",
      "SCORE: : [('vclaim-pol-00000', 0.16315478086471558), ('vclaim-pol-00001', 0.07074496150016785)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-144-2c3b9df55c6c>:13: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  score.append((vclaims_list[encoddings_list.index(encodding)]['vclaim_id'], average))\n"
     ]
    }
   ],
   "source": [
    "# Compute scores by finding cosine similarity between all claims and the query\n",
    "\n",
    "score = []\n",
    "\n",
    "for encodding in encoddings_list:\n",
    "    print(encodding.shape)\n",
    "    results = util.semantic_search(query, encodding, top_k=5)\n",
    "    result_sum = 0\n",
    "    for result in results:\n",
    "        print(result)\n",
    "        result_sum+=result[0]['score']\n",
    "    average = result_sum / len(results)\n",
    "    score.append((vclaims_list[encoddings_list.index(encodding)]['vclaim_id'], average))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"SCORE: :\",score)\n",
    "\n",
    "\n",
    "\n",
    "# score = []\n",
    "\n",
    "# for result in results:\n",
    "#     score.append(())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "essential-metadata",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(iclaims, vclaims_list, index, search_keys, size):\n",
    "    iclaims_count, vclaims_count = len(iclaims), len(vclaims_list)\n",
    "    scores = {}\n",
    "\n",
    "    logging.info(f\"Geting RM5 scores for {iclaims_count} iclaims and {vclaims_count} vclaims\")\n",
    "\n",
    "    for iclaim_id, iclaim in iclaims:\n",
    "        score = get_score(iclaim, vclaims_list, index, search_keys=search_keys, size=size)\n",
    "        scores[iclaim_id] = score\n",
    "    return scores\n",
    "\n",
    "\n",
    "def format_scores(scores):\n",
    "    output_string = ''\n",
    "    for iclaim_id in scores:\n",
    "        for i, (vclaim_id, score) in enumerate(scores[iclaim_id]):\n",
    "            output_string += f\"{iclaim_id}\\tQ0\\t{vclaim_id}\\t{i + 1}\\t{score}\\telasic\\n\"\n",
    "    return output_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quick-migration",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_baselines(args):\n",
    "    if not exists('baselines/data'):\n",
    "        os.mkdir('baselines/data')\n",
    "    vclaims, vclaims_list = load_vclaims(args.vclaims_dir_path)\n",
    "    all_iclaims = pd.read_csv(args.iclaims_file_path, sep='\\t', names=['iclaim_id', 'iclaim'])\n",
    "    wanted_iclaim_ids = pd.read_csv(args.dev_file_path, sep='\\t', names=['iclaim_id', '0', 'vclaim_id', 'relevance'])\n",
    "    wanted_iclaim_ids = wanted_iclaim_ids.iclaim_id.tolist()\n",
    "\n",
    "    iclaims = []\n",
    "    for iclaim_id in wanted_iclaim_ids:\n",
    "        iclaim = all_iclaims.iclaim[all_iclaims.iclaim_id == iclaim_id].iloc[0]\n",
    "        iclaims.append((iclaim_id, iclaim))\n",
    "\n",
    "    index = f\"{args.subtask}-{args.lang}\"\n",
    "\n",
    "    # options are title, vclaim, text\n",
    "    scores = get_scores(iclaims, vclaims_list, index, search_keys=args.keys, size=args.size)\n",
    "    ngram_baseline_fpath = join(ROOT_DIR,\n",
    "                                f'baselines/data/subtask_{args.subtask}_bm25_{args.lang}_{basename(args.dev_file_path)}')\n",
    "    formatted_scores = format_scores(scores)\n",
    "    with open(ngram_baseline_fpath, 'w') as f:\n",
    "        f.write(formatted_scores)\n",
    "    maps, mrr, precisions = evaluate(args.dev_file_path, ngram_baseline_fpath)\n",
    "    logging.info(f\"S-BERT Baseline for Subtask-{args.subtask}--{args.lang}\")\n",
    "    logging.info(f'All MAP scores on threshold from [1, 3, 5, 10, 20, 50, 1000]. {maps}')\n",
    "    logging.info(f'MRR score {mrr}')\n",
    "    logging.info(f'All P scores on threshold from [1, 3, 5, 10, 20, 50, 1000]. {precisions}')\n",
    "\n",
    "\n",
    "# python baselines/bm25.py --train-file-path=baselines/v1/train.tsv --dev-file-path=baselines/v1/train.tsv --vclaims-dir-path=baselines/politifact-vclaims --iclaims-file-path=baselines/v1/iclaims.queries --subtask=2b --lang=english\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--train-file-path\", \"-t\", required=True, type=str,\n",
    "                        help=\"The absolute path to the training data\")\n",
    "    parser.add_argument(\"--dev-file-path\", \"-d\", required=True, type=str,\n",
    "                        help=\"The absolute path to the dev data\")\n",
    "    parser.add_argument(\"--vclaims-dir-path\", \"-v\", required=True, type=str,\n",
    "                        help=\"The absolute path to the directory with the verified claim documents\")\n",
    "    parser.add_argument(\"--iclaims-file-path\", \"-i\", required=True,\n",
    "                        help=\"TSV file with iclaims. Format: iclaim_id iclaim_content\")\n",
    "    parser.add_argument(\"--keys\", \"-k\", default=['vclaim', 'title'],\n",
    "                        help=\"Keys to search in the document\")\n",
    "    parser.add_argument(\"--size\", \"-s\", default=19250,\n",
    "                        help=\"Maximum results extracted for a query\")\n",
    "    parser.add_argument(\"--subtask\", \"-m\", required=True,\n",
    "                        choices=['2a', '2b'],\n",
    "                        help=\"The subtask you want to check the format of.\")\n",
    "    parser.add_argument(\"--lang\", \"-l\", required=True, type=str,\n",
    "                        choices=['arabic', 'english'],\n",
    "                        help=\"The language of the subtask\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    run_baselines(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
